{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b9292b",
   "metadata": {},
   "source": [
    "# Nueral architecture search on minst dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf10136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 21:44:23.180790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 21:44:24.013225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:44:24.013286: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 21:44:25.820364: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:44:25.820441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:44:25.820448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import autokeras as ak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1ecf0",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "Here load the mnist data into training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a58f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "[5 0 4]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n",
    "print(y_train[:3])  # array([7, 2, 1], dtype=uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f421d",
   "metadata": {},
   "source": [
    "# Training\n",
    " Here train the autokeras built in image clasfier with mnist data after the training the validation accuracy of the architecture is 98 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08373714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 06m 51s]\n",
      "val_loss: 0.042387738823890686\n",
      "\n",
      "Best val_loss So Far: 0.042387738823890686\n",
      "Total elapsed time: 00h 06m 51s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1546 - accuracy: 0.9532\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0721 - accuracy: 0.9779\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0573 - accuracy: 0.9822\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0505 - accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0436 - accuracy: 0.9863\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.0403 - accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0353 - accuracy: 0.9894\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0349 - accuracy: 0.9888\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0317 - accuracy: 0.9891\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0317 - accuracy: 0.9901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n",
      "313/313 [==============================] - 2s 5ms/step\n",
      "[['7']\n",
      " ['2']\n",
      " ['1']\n",
      " ...\n",
      " ['4']\n",
      " ['5']\n",
      " ['6']]\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0362 - accuracy: 0.9887\n",
      "[0.03616759181022644, 0.9886999726295471]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the image classifier.\n",
    "clf = ak.ImageClassifier(overwrite=True, max_trials=1)\n",
    "# Feed the image classifier with training data.\n",
    "clf.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)\n",
    "print(predicted_y)\n",
    "\n",
    "\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e5758",
   "metadata": {},
   "source": [
    "# Customized Search Space\n",
    "\n",
    "here only resnet archirtures will be searched for classification on mnist data will be train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774db8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock(\n",
    "    # Only search ResNet architectures.\n",
    "    block_type=\"resnet\",\n",
    "    # Normalize the dataset.\n",
    "    normalize=True,\n",
    "    # Do not do data augmentation.\n",
    "    augment=False,\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcae16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [04h 43m 01s]\n",
      "val_loss: 0.06486619263887405\n",
      "\n",
      "Best val_loss So Far: 0.06486619263887405\n",
      "Total elapsed time: 04h 43m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2046s 1s/step - loss: 0.3450 - accuracy: 0.9168\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2036s 1s/step - loss: 0.1741 - accuracy: 0.9604\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2039s 1s/step - loss: 0.2676 - accuracy: 0.9439\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2040s 1s/step - loss: 0.1767 - accuracy: 0.9560\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2045s 1s/step - loss: 0.2132 - accuracy: 0.9497\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2043s 1s/step - loss: 0.1179 - accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2046s 1s/step - loss: 0.1320 - accuracy: 0.9692\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2049s 1s/step - loss: 0.0977 - accuracy: 0.9760\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2050s 1s/step - loss: 0.0555 - accuracy: 0.9852\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2050s 1s/step - loss: 0.0573 - accuracy: 0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b0c3cfee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6999e36",
   "metadata": {},
   "source": [
    "Here by using AutoKeras to create an Image Classification model, but with a slightly different custom search space like The ResNetBlock layer is added to the output node. In this case, the version argument is set to \"v2\", which specifies the version of the ResNet architecture to use. An AutoModel is created using the input and output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aff484",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = ak.ImageInput()\n",
    "output_node = ak.Normalization()(input_node)\n",
    "output_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\n",
    "output_node = ak.ResNetBlock(version=\"v2\")(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfca334",
   "metadata": {},
   "source": [
    "The overwrite argument is set to True, which means that if there is an existing model with the same name, it will be overwritten. The max_trials argument is set to 1, which means that AutoKeras will run 1 trials to find the best model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64fe019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [07h 48m 30s]\n",
      "val_loss: 4.578865051269531\n",
      "\n",
      "Best val_loss So Far: 4.578865051269531\n",
      "Total elapsed time: 07h 48m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3580s 2s/step - loss: 0.2054 - accuracy: 0.9427\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3548s 2s/step - loss: 0.1375 - accuracy: 0.9671\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4527s 2s/step - loss: 0.1028 - accuracy: 0.9740\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3477s 2s/step - loss: 0.0775 - accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3392s 2s/step - loss: 0.0682 - accuracy: 0.9813\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3394s 2s/step - loss: 0.0642 - accuracy: 0.9837\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3395s 2s/step - loss: 0.0425 - accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5160s 3s/step - loss: 0.0364 - accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5909s 3s/step - loss: 0.0310 - accuracy: 0.9911\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5899s 3s/step - loss: 0.0275 - accuracy: 0.9921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 105). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a9a12a790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d313d0a",
   "metadata": {},
   "source": [
    "# AutoKeras image classifier for STL10 \n",
    "For train the image classifier on STL10 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470519c",
   "metadata": {},
   "source": [
    "Import the necessary libraries: numpy, torch, torchvision.datasets, torchvision.transforms, and autokeras.ImageClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2ea435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 07:31:15.838983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 07:31:18.406255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 07:31:18.406376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 07:31:18.406383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from autokeras import ImageClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd8e3f",
   "metadata": {},
   "source": [
    "Check if a GPU is available and set the device to GPU if it is available; otherwise, set the device to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae52f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abd1b1",
   "metadata": {},
   "source": [
    "Define the transformation to be applied to the images in the dataset. In this case, the images will be converted to tensors and normalized with a mean of 0.5 and a standard deviation of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cdd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3d9d1",
   "metadata": {},
   "source": [
    "Load the STL10 dataset using the DataLoader class from torchvision.datasets. The dataset is split into a training set and a test set. The train_loader and test_loader are created using the DataLoader class with the dataset, batch size of 6, and shuffle set to True for the training set and False for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495aa890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the STL10 dataset using the DataLoader class\n",
    "train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
    "test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1caff",
   "metadata": {},
   "source": [
    "Extract the data and labels from the train_loader and test_loader and concatenate them into x_train, y_train, x_test, and y_test arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494a1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and labels from the loaders\n",
    "x_train = []\n",
    "y_train = []\n",
    "for data, label in train_loader:\n",
    "    x_train.append(data.numpy())\n",
    "    y_train.append(label.numpy())\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for data, label in test_loader:\n",
    "    x_test.append(data.numpy())\n",
    "    y_test.append(label.numpy())\n",
    "x_test = np.concatenate(x_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "# Move the data to GPU if available\n",
    "#x_train = torch.from_numpy(x_train).to(device)\n",
    "#y_train = torch.from_numpy(y_train).to(device)\n",
    "#x_test = torch.from_numpy(x_test).to(device)\n",
    "#y_test = torch.from_numpy(y_test).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1186eb5",
   "metadata": {},
   "source": [
    "Define the ImageClassifier model from AutoKeras with a max_trials value of 1, which means that AutoKeras will only try one set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "clf = ImageClassifier(max_trials=1, overwrite=True, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4e182",
   "metadata": {},
   "source": [
    "Train the model using the fit method with the x_train and y_train arrays and 10 epochs. Then evaluate the model on the test data using the evaluate method with the x_test and y_test arrays.\n",
    "Finally print the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e954a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 26s]\n",
      "val_loss: 1.6100943088531494\n",
      "\n",
      "Best val_loss So Far: 1.6100943088531494\n",
      "Total elapsed time: 00h 01m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 1.9860 - accuracy: 0.2772\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 9s 54ms/step - loss: 1.7174 - accuracy: 0.3796\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 8s 51ms/step - loss: 1.6180 - accuracy: 0.4186\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 1.5042 - accuracy: 0.4526\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 8s 52ms/step - loss: 1.4325 - accuracy: 0.4816\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 8s 54ms/step - loss: 1.3742 - accuracy: 0.5010\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 8s 52ms/step - loss: 1.3211 - accuracy: 0.5178\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 1.2500 - accuracy: 0.5562\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 1.2082 - accuracy: 0.5628\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 8s 52ms/step - loss: 1.1869 - accuracy: 0.5686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 5s 13ms/step - loss: 1.5903 - accuracy: 0.4604\n",
      "Test accuracy: 0.46037501096725464\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "clf.fit(x_train, y_train, epochs=10)\n",
    "# Evaluate the model\n",
    "score = clf.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ca77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 5s 13ms/step - loss: 1.5903 - accuracy: 0.4604\n",
      "Accuracy: 46.04%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the AutoModel on the test data\n",
    "accuracy = clf.evaluate(x_test, y_test)[1]\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b606c",
   "metadata": {},
   "source": [
    "Here Export Model and print the summary of model. The output shows that the exported model is a Keras functional model, which is a type of model that allows for more complex architectures than the simple sequential model.\n",
    "\n",
    "The model has several layers, including convolutional layers, pooling layers, and a dense layer, with a total of 107,819 parameters. The model is trained to classify the STL-10 dataset into 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16aed8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 96, 96)]       0         \n",
      "                                                                 \n",
      " cast_to_float32 (CastToFloa  (None, 3, 96, 96)        0         \n",
      " t32)                                                            \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 3, 96, 96)        193       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 3, 96, 32)         27680     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 3, 96, 64)         18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 2, 48, 64)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 48, 64)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6144)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                61450     \n",
      "                                                                 \n",
      " classification_head_1 (Soft  (None, 10)               0         \n",
      " max)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,819\n",
      "Trainable params: 107,626\n",
      "Non-trainable params: 193\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Export as a Keras Model.\n",
    "model = clf.export_model()\n",
    "#print(model)\n",
    "print(type(model))  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59041d0",
   "metadata": {},
   "source": [
    "# AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf9c53",
   "metadata": {},
   "source": [
    "Define the AutoModel using the ImageInput and ImageBlock classes from AutoKeras to define the input and convolutional layers of the neural network respectively. Then add a Flatten layer to convert the output of the convolutional layers into a 1D tensor, and a DenseBlock to add some fully connected layers to the network. Finally, add a ClassificationHead layer to the network to output the classification probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7045d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "# Define the AutoModel\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock()(input_node)\n",
    "output_node = ak.Flatten()(output_node)\n",
    "output_node = ak.DenseBlock()(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670f925",
   "metadata": {},
   "source": [
    "Initialize the AutoModel by passing in the input and output nodes defined in the previous step, along with some additional parameters. Set overwrite to True to allow the model to overwrite existing files, and set max_trials to 2 to limit the number of different architectures that the model will try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c89ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the AutoModel\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    overwrite=True,\n",
    "    max_trials=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c58733",
   "metadata": {},
   "source": [
    "Train the AutoModel using the fit method, passing in the training data x_train and y_train, and the number of epochs to train for.Evaluate the AutoModel on the test data using the evaluate method, passing in x_test and y_test. Save the resulting accuracy score in a variable named accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49f00fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 33m 27s]\n",
      "val_loss: 2.3032314777374268\n",
      "\n",
      "Best val_loss So Far: 2.3030271530151367\n",
      "Total elapsed time: 01h 02m 03s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 379s 2s/step - loss: 2.3724 - accuracy: 0.0924\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 362s 2s/step - loss: 2.3029 - accuracy: 0.0970\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 364s 2s/step - loss: 2.3028 - accuracy: 0.0996\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 364s 2s/step - loss: 2.3029 - accuracy: 0.0932\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 362s 2s/step - loss: 2.3029 - accuracy: 0.0914\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 360s 2s/step - loss: 2.3028 - accuracy: 0.0984\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 363s 2s/step - loss: 2.3029 - accuracy: 0.0950\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 363s 2s/step - loss: 2.3029 - accuracy: 0.0978\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 363s 2s/step - loss: 2.3029 - accuracy: 0.0952\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 365s 2s/step - loss: 2.3029 - accuracy: 0.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 55). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n",
      "2023-03-30 09:38:00.994674: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 884736000 exceeds 10% of free system memory.\n",
      "2023-03-30 09:38:12.569548: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 884736000 exceeds 10% of free system memory.\n",
      "2023-03-30 09:38:13.241355: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 884736000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 53s 198ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train, epochs=10)\n",
    "# Evaluate the AutoModel on the test data\n",
    "accuracy = clf.evaluate(x_test, y_test)[1]\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167f4ac",
   "metadata": {},
   "source": [
    "Here is the summary of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a69e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 96, 96)]       0         \n",
      "                                                                 \n",
      " cast_to_float32 (CastToFloa  (None, 3, 96, 96)        0         \n",
      " t32)                                                            \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 32, 96, 96)        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 96, 3)         291       \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 1, 3, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6144)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                196640    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      " classification_head_2 (Soft  (None, 10)               0         \n",
      " max)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,786,029\n",
      "Trainable params: 23,732,909\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Export as a Keras Model.\n",
    "model = clf.export_model()\n",
    "#print(model)\n",
    "print(type(model))  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf4239",
   "metadata": {},
   "source": [
    "# For DTD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3b38d",
   "metadata": {},
   "source": [
    " # AutoModel\n",
    "Here the task is Image classification using AutoKeras, Specifically an image classifier using a ResNet architecture on DTD and utilizes AutoKeras to automatically search for the best hyperparameters and architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a392fa",
   "metadata": {},
   "source": [
    "Import necessary libraries including numpy, torch, datasets and transforms from torchvision, and the ImageClassifier from autokeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65df6846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 17:21:25.343334: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 17:21:28.136195: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 17:21:28.136272: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 17:21:28.136279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from autokeras import ImageClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d75510",
   "metadata": {},
   "source": [
    "Check if a GPU is available and if not, fall back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available, if not fall back to CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8bd56",
   "metadata": {},
   "source": [
    "Define the transform for the dataset, including resizing the images to 224x224 pixels, converting them to tensors, and normalizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1661bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # add resize operation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c0059",
   "metadata": {},
   "source": [
    "Load the train and test datasets using the ImageFolder class from PyTorch's torchvision module and create data loaders using the DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a81448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DTD dataset using the DataLoader class\n",
    "train_dataset = datasets.ImageFolder(root='/media/abdul/8266fdc5-61f6-47d6-9e82-aa0e56f4b319/Ilyas/classification/DTD/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='/media/abdul/8266fdc5-61f6-47d6-9e82-aa0e56f4b319/Ilyas/classification/DTD/val', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: torch.utils.data.dataloader.default_collate([y for y in x]))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: torch.utils.data.dataloader.default_collate([y for y in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1113ee9",
   "metadata": {},
   "source": [
    "Concatenate the data and labels from the loaders into numpy arrays for use with AutoKeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df90bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and labels from the loaders\n",
    "x_train = np.concatenate([data.numpy() for data, label in train_loader], axis=0)\n",
    "y_train = np.concatenate([label.numpy() for data, label in train_loader], axis=0)\n",
    "\n",
    "x_test = np.concatenate([data.numpy() for data, label in test_loader], axis=0)\n",
    "y_test = np.concatenate([label.numpy() for data, label in test_loader], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62313687",
   "metadata": {},
   "source": [
    "Define the AutoKeras model using an ImageInput node, an ImageBlock node with a ResNet architecture and no data augmentation, and a ClassificationHead node.Use the AutoModel class to create the model and fit it to the training data for 10 epochs with a maximum of 3 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bedf6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock(\n",
    "    # Only search ResNet architectures.\n",
    "    block_type=\"resnet\",\n",
    "    # Normalize the dataset.\n",
    "    normalize=True,\n",
    "    # Do not do data augmentation.\n",
    "    augment=False,\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 59m 49s]\n",
      "val_loss: 3.8677918910980225\n",
      "\n",
      "Best val_loss So Far: 3.8677918910980225\n",
      "Total elapsed time: 00h 59m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538248c",
   "metadata": {},
   "source": [
    "Export the model as a Keras model and print its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acced54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as a Keras Model.\n",
    "model = clf.export_model()\n",
    "#print(model)\n",
    "print(type(model))  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0d878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
